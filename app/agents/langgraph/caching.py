"""
LangGraph Booking Agent - Optimized Resource Caching
====================================================

Multi-level caching strategy to reduce API calls and improve performance.

CACHING STRATEGY:
=================

Level 1: Memory Cache (In-Process)
- Fastest access (~1Âµs)
- Cleared on restart
- TTL: 5 minutes
- Use for: Frequently accessed data

Level 2: Redis Cache (Shared)
- Fast access (~1ms)
- Persists across restarts
- TTL: 5 minutes
- Use for: Shared data across instances

Level 3: API Call (External)
- Slowest access (~500ms)
- Always fresh data
- Use when: Cache miss

PERFORMANCE IMPACT:
===================
Without Cache:
- Service types fetch: 500ms x 100 requests = 50s total
- Doctors fetch: 700ms x 100 requests = 70s total

With Cache:
- Service types fetch: 500ms (first) + 1ms x 99 = 600ms total (83% faster)
- Doctors fetch: 700ms (first) + 1ms x 99 = 800ms total (89% faster)

CACHE KEYS:
===========
- service_types: All service type categories
- services_type_{id}: Services for specific type
- doctors_service_{id}: Doctors for specific service
- doctors_all: All doctors in system
- devices_all: All devices in system
- specialists_all: All specialists in system

Author: LangGraph Migration Team
Date: October 2025
"""
import json
import time
from typing import List, Dict, Optional
from loguru import logger


class CachedResourceLoader:
    """
    Optimized resource loading with multi-level caching.
    
    Reduces API calls by caching service types, services, doctors, etc.
    Implements 3-level cache: Memory â†’ Redis â†’ API
    """
    
    def __init__(self, api_client, redis_client):
        """
        Initialize cached resource loader.
        
        Args:
            api_client: AgentApiClient instance
            redis_client: Redis client for L2 cache
        """
        self.api_client = api_client
        self.redis_client = redis_client
        self.memory_cache: Dict[str, tuple] = {}  # {key: (data, timestamp)}
        self.cache_ttl = 300  # 5 minutes
        
        # Cache statistics
        self.stats = {
            "memory_hits": 0,
            "redis_hits": 0,
            "api_calls": 0
        }
    
    async def get_service_types(self) -> List[dict]:
        """
        Get service types with 3-level caching.
        
        Performance:
        - Memory hit: ~1Âµs
        - Redis hit: ~1ms
        - API call: ~500ms
        
        Returns:
            List of service type dicts
        """
        cache_key = "service_types"
        
        # Level 1: Memory cache
        if cache_key in self.memory_cache:
            cached_data, timestamp = self.memory_cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                self.stats["memory_hits"] += 1
                logger.debug(f"âš¡ L1 CACHE HIT: service_types (memory)")
                return cached_data
        
        # Level 2: Redis cache
        redis_key = f"booking_cache:{cache_key}"
        try:
            cached_json = await self.redis_client.get(redis_key)
            if cached_json:
                self.stats["redis_hits"] += 1
                logger.info(f"âš¡ L2 CACHE HIT: service_types (Redis)")
                data = json.loads(cached_json)
                # Populate memory cache
                self.memory_cache[cache_key] = (data, time.time())
                return data
        except Exception as e:
            logger.warning(f"âš ï¸ Redis cache read error: {e}")
        
        # Level 3: API call
        self.stats["api_calls"] += 1
        logger.info(f"ðŸ”€ CACHE MISS: Fetching service_types from API (L3)")
        
        from ..service_flow_helpers import fetch_service_types
        data = await fetch_service_types(self.api_client)
        
        # Store in both caches
        self.memory_cache[cache_key] = (data, time.time())
        try:
            await self.redis_client.setex(
                redis_key,
                self.cache_ttl,
                json.dumps(data)
            )
            logger.debug(f"âœ… Cached service_types (L1 + L2)")
        except Exception as e:
            logger.warning(f"âš ï¸ Redis cache write error: {e}")
        
        return data
    
    async def get_services_by_type(self, type_id: int) -> List[dict]:
        """
        Get services for specific service type with caching.
        
        Args:
            type_id: Service type ID
            
        Returns:
            List of service dicts
        """
        cache_key = f"services_type_{type_id}"
        
        # Level 1: Memory cache
        if cache_key in self.memory_cache:
            cached_data, timestamp = self.memory_cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                self.stats["memory_hits"] += 1
                logger.debug(f"âš¡ L1 CACHE HIT: services_type_{type_id}")
                return cached_data
        
        # Level 2: Redis cache
        redis_key = f"booking_cache:{cache_key}"
        try:
            cached_json = await self.redis_client.get(redis_key)
            if cached_json:
                self.stats["redis_hits"] += 1
                logger.info(f"âš¡ L2 CACHE HIT: services_type_{type_id}")
                data = json.loads(cached_json)
                self.memory_cache[cache_key] = (data, time.time())
                return data
        except Exception as e:
            logger.warning(f"âš ï¸ Redis cache read error: {e}")
        
        # Level 3: API call
        self.stats["api_calls"] += 1
        logger.info(f"ðŸ”€ CACHE MISS: Fetching services for type {type_id} (L3)")
        
        from ..service_flow_helpers import fetch_services_by_type
        data = await fetch_services_by_type(self.api_client, type_id)
        
        # Store in both caches
        self.memory_cache[cache_key] = (data, time.time())
        try:
            await self.redis_client.setex(
                redis_key,
                self.cache_ttl,
                json.dumps(data)
            )
        except Exception as e:
            logger.warning(f"âš ï¸ Redis cache write error: {e}")
        
        return data
    
    async def get_doctors(self, service_id: Optional[int] = None) -> List[dict]:
        """
        Get doctors with caching.
        
        Args:
            service_id: Optional service ID to filter doctors
            
        Returns:
            List of doctor dicts
        """
        cache_key = f"doctors_service_{service_id}" if service_id else "doctors_all"
        
        # Level 1: Memory cache
        if cache_key in self.memory_cache:
            cached_data, timestamp = self.memory_cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                self.stats["memory_hits"] += 1
                logger.debug(f"âš¡ L1 CACHE HIT: {cache_key}")
                return cached_data
        
        # Level 2: Redis cache
        redis_key = f"booking_cache:{cache_key}"
        try:
            cached_json = await self.redis_client.get(redis_key)
            if cached_json:
                self.stats["redis_hits"] += 1
                logger.info(f"âš¡ L2 CACHE HIT: {cache_key}")
                data = json.loads(cached_json)
                self.memory_cache[cache_key] = (data, time.time())
                return data
        except Exception as e:
            logger.warning(f"âš ï¸ Redis cache read error: {e}")
        
        # Level 3: API call
        self.stats["api_calls"] += 1
        logger.info(f"ðŸ”€ CACHE MISS: Fetching {cache_key} (L3)")
        
        if service_id:
            result = await self.api_client.get(f"/services/{service_id}/doctors")
        else:
            result = await self.api_client.get("/doctors", params={"limit": 20})
        
        data = result.get("results") or result.get("data") or []
        
        # Store in both caches
        self.memory_cache[cache_key] = (data, time.time())
        try:
            await self.redis_client.setex(
                redis_key,
                self.cache_ttl,
                json.dumps(data)
            )
        except Exception as e:
            logger.warning(f"âš ï¸ Redis cache write error: {e}")
        
        return data
    
    async def invalidate_cache(self, pattern: Optional[str] = None):
        """
        Invalidate cache entries.
        
        Args:
            pattern: Optional pattern to match (e.g., "services_*")
                    If None, clears all caches
        """
        if pattern:
            # Clear specific pattern from Redis
            try:
                keys = []
                async for key in self.redis_client.scan_iter(f"booking_cache:{pattern}*"):
                    keys.append(key)
                
                if keys:
                    await self.redis_client.delete(*keys)
                    logger.info(f"ðŸ—‘ï¸ Invalidated Redis cache: {pattern} ({len(keys)} keys)")
            except Exception as e:
                logger.error(f"âŒ Redis cache invalidation error: {e}")
            
            # Clear matching keys from memory cache
            keys_to_remove = [k for k in self.memory_cache.keys() if pattern in k]
            for key in keys_to_remove:
                del self.memory_cache[key]
            
            if keys_to_remove:
                logger.info(f"ðŸ—‘ï¸ Invalidated memory cache: {pattern} ({len(keys_to_remove)} keys)")
        else:
            # Clear all caches
            try:
                keys = []
                async for key in self.redis_client.scan_iter("booking_cache:*"):
                    keys.append(key)
                
                if keys:
                    await self.redis_client.delete(*keys)
                    logger.info(f"ðŸ—‘ï¸ Cleared all Redis cache ({len(keys)} keys)")
            except Exception as e:
                logger.error(f"âŒ Redis cache clear error: {e}")
            
            self.memory_cache.clear()
            logger.info("ðŸ—‘ï¸ Cleared all memory cache")
    
    def get_stats(self) -> Dict[str, int]:
        """
        Get cache performance statistics.
        
        Returns:
            Dict with hit/miss counts
        """
        total = sum(self.stats.values())
        if total == 0:
            return {
                **self.stats,
                "total": 0,
                "hit_rate": 0.0
            }
        
        hit_rate = (self.stats["memory_hits"] + self.stats["redis_hits"]) / total * 100
        
        return {
            **self.stats,
            "total": total,
            "hit_rate": round(hit_rate, 2)
        }
    
    def reset_stats(self):
        """Reset cache statistics"""
        self.stats = {
            "memory_hits": 0,
            "redis_hits": 0,
            "api_calls": 0
        }
